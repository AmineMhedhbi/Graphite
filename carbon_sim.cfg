# Configuration file for the carbon simulator
[general]
output_dir = "./output_files/"
output_file = "sim.out"

# Total number of cores in all processes
total_cores = 64
num_processes = 1

enable_dcache_modeling = true
enable_icache_modeling = false
enable_performance_modeling = true

enable_shared_mem = true
enable_syscall_modeling = true

[transport]
base_port = 2000

[log]
enabled = false
stack_trace = false
disabled_modules = ""
#enabled_modules = "l1_cache_cntlr.cc memory_manager.cc network.cc core.cc"
enabled_modules = ""

[progress_trace]
enabled = false
interval = 5000

[clock_skew_minimization]
scheme = none  # Valid Schemes are 'none,barrier,random_pairs,ring'

[clock_skew_minimization/barrier]
quantum = 1000    # Synchronize after every quantum
[clock_skew_minimization/random_pairs]
quantum = 100000   # Could be equal to slack but kept different for generality
slack = 100000
sleep_fraction = 0.4   # Equal to the fraction of computed time the core sleeps
[clock_skew_minimization/ring]
slack = 1000       # Messages could be sent on the ring after a delay. Not shown here

[stack]
stack_base = 2415919104
stack_size_per_core = 4194304

[process_map]
process0 = "127.0.0.1"
process1 = "127.0.0.1"
process2 = "127.0.0.1"
process3 = "127.0.0.1"
process4 = "127.0.0.1"
process5 = "127.0.0.1"
process6 = "127.0.0.1"
process7 = "127.0.0.1"
process8 = "127.0.0.1"
process9 = "127.0.0.1"
process10 = "127.0.0.1"
process11 = "127.0.0.1"
process12 = "127.0.0.1"
process13 = "127.0.0.1"
process14 = "127.0.0.1"
process15 = "127.0.0.1"
process16 = "127.0.0.1"

[perf_model/core]
frequency = 3.16 # In GHz
type = simple

[perf_model/core/iocoom]
num_store_buffer_entries = 20
num_outstanding_loads = 32

[perf_model/core/static_instruction_costs]
add=1
sub=1
mul=3
div=18
fadd=3
fsub=3
fmul=5
fdiv=6
generic=1
jmp=1

[perf_model/branch_predictor]
type=one_bit
mispredict_penalty=14 # A guess based on Penryn pipeline depth
size=1024

[perf_model/l1_icache]
enable = true
cache_block_size = 64
cache_size = 32 # in KB
associativity = 8
replacement_policy = lru
data_access_time = 3
tags_access_time = 1
perf_model_type = parallel

[perf_model/l1_dcache]
enable = true
cache_block_size = 64
cache_size = 32 # in KB
associativity = 8
replacement_policy = lru 
data_access_time = 3
tags_access_time = 1
perf_model_type = parallel

[perf_model/l2_cache]
enable = true
cache_block_size = 64
cache_size = 3072 # in KB
associativity = 24
replacement_policy = lru  # Not documented but I'm guessing pseudo-LRU
data_access_time = 15
tags_access_time = 4  # This is just a guess for Penryn
perf_model_type = parallel

[caching_protocol]
type = pr_l1_pr_l2_dram_directory_msi

# TODO, Does not belong here, take me out
[caching_protocol/pr_l1_pr_l2_dram_directory_mosi]
# If number of hops (as calculated in an electrical mesh) in unicast is less than
# unicast_threshold, then packets are sent on 'unicast_network_type_lt_threshold',
# else, packets are sent on 'unicast_network_type_ge_threshold'.
# Broadcast packets are always sent on 'broadcast_network_type'
unicast_threshold = 4
unicast_network_type_lt_threshold = memory_model_1
unicast_network_type_ge_threshold = memory_model_2
broadcast_network_type = memory_model_2

[perf_model/dram_directory]
total_entries = 24576
associativity = 24
max_hw_sharers = 64 # number of sharers supported in hardware
directory_type = limited_broadcast
home_lookup_param = 8
directory_cache_access_time = 10
[perf_model/dram_directory/limitless]
software_trap_penalty = 200 # number of cycles added to clock when trapping into software (pulled number from Chaiken papers, which explores 25-150 cycle penalties)

[perf_model/dram]
latency = 75 # In nanoseconds
per_controller_bandwidth = 5 # In GB/s
num_controllers = -1 # Total Bandwidth = per_controller_bandwidth * num_controllers
controller_positions = ""
[perf_model/dram/queue_model]
enabled = true
type = history_list

[network]
user_model_1 = emesh_hop_counter
user_model_2 = emesh_hop_counter
memory_model_1 = emesh_hop_counter
memory_model_2 = emesh_hop_counter
system_model = magic

# see comments in network_model_analytical.cc
[network/analytical]
Tw2 = 1
s = 1
n = 1
W = 32
update_interval = 1000
processing_cost = 100

[network/emesh_hop_counter]
link_bandwidth = 64 # In bits/cycles
hop_latency = 2

[network/emesh_hop_by_hop_basic]
link_bandwidth = 128 # In bits/cycle
hop_latency = 2 # In cycles
[network/emesh_hop_by_hop_basic/queue_model]
enabled = true
type = history_list

[network/emesh_hop_by_hop_broadcast_tree]
link_bandwidth = 64 # In bits/cycle
hop_latency = 4 # In cycles
[network/emesh_hop_by_hop_broadcast_tree/queue_model]
enabled = true
type = history_list

[network/atac_optical_bus]
# Specify both the below values only in integers
bandwidth = 64 # 64 bits/cycle from each core - 64 waveguides total
latency = 3 # 3 clock cycles
[network/atac_optical_bus/queue_model]
enabled = false
type = history_list

[queue_model/basic]
moving_avg_enabled = true
moving_avg_window_size = 1024
moving_avg_type = arithmetic_mean

[queue_model/history_list]
# Uses the analytical model (if enabled) to calculate delay if cannot be calculated using the history list
max_list_size = 100
analytical_model_enabled = true
